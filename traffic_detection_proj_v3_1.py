# -*- coding: utf-8 -*-
"""Traffic_Detection_Proj_V3.1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zyEfOdBGlYnDB_GCvB4-U60v0SUPwzpd
"""

# Check PyTorch and CUDA versions
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
!nvcc --version

# Check if installation succeeded, fallback to source if failed
try:
    import detectron2
    print(f"Detectron2 version: {detectron2.__version__}")
except ImportError:
    print("Wheel installation failed, building from source...")
    !pip install 'git+https://github.com/facebookresearch/detectron2.git@main'
    # Restart runtime after source install
    import os
    os.kill(os.getpid(), 9)  # Forces runtime restart

# Import and verify after restart
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()
print(f"Detectron2 version: {detectron2.__version__}")

"""# COCO Vehicle Classes Preprocessing Pipeline
This script extracts and balances vehicle classes from COCO dataset for traffic detection
"""

import os
import gc
import cv2
import json
import copy
import torch
import random
import detectron2
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from collections import defaultdict
import albumentations as A

# For COCO dataset
from pycocotools import mask as mask_util
from pycocotools.coco import COCO

# Required for instance segmentation data formatting
from detectron2.structures import BoxMode

# Set random seed for reproducibility
random.seed(69)
np.random.seed(69)
torch.manual_seed(69)

# Configuration
COCO_DIR = "/content/coco2017"
TRAIN_DIR = os.path.join(COCO_DIR, "train2017")
VAL_DIR = os.path.join(COCO_DIR, "val2017")
TRAIN_JSON = os.path.join(COCO_DIR, "annotations/instances_train2017.json")
VAL_JSON = os.path.join(COCO_DIR, "annotations/instances_val2017.json")
OUTPUT_DIR = os.path.join(COCO_DIR, "vehicle_dataset")

# Create output directories
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Image parameters
IMG_SIZE = 512

# Vehicle class mapping
VEHICLE_CLASSES = ["car", "non-car"]
CORRECT_COCO_MAPPING = {
    3: 0,  # COCO car -> class 0
    2: 1,   # bicycle -> non-car
    4: 1,   # motorcycle -> non-car
    6: 1,   # bus -> non-car
    8: 1    # truck -> non-car
}

def download_and_extract_coco():
    """Download and extract COCO dataset including test set"""
    os.makedirs(COCO_DIR, exist_ok=True)

    # Check if files exist
    if os.path.exists(TRAIN_DIR) and os.path.exists(VAL_DIR) and os.path.exists(TRAIN_JSON):
        print("COCO dataset already exists")
        return

    print("Downloading and extracting COCO dataset...")

    # Download commands
    !wget http://images.cocodataset.org/zips/train2017.zip -P {COCO_DIR}
    !wget http://images.cocodataset.org/zips/val2017.zip -P {COCO_DIR}
    !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P {COCO_DIR}

    # Unzip
    !unzip {COCO_DIR}/train2017.zip -d {COCO_DIR}/
    !unzip {COCO_DIR}/val2017.zip -d {COCO_DIR}/
    !unzip {COCO_DIR}/test2017.zip -d {COCO_DIR}/
    !unzip {COCO_DIR}/annotations_trainval2017.zip -d {COCO_DIR}/

    # Cleanup
    !rm {COCO_DIR}/*.zip

    print("COCO dataset downloaded and extracted successfully!")

download_and_extract_coco()

!find /content/coco2017/train2017 -name "*.jpg" | wc -l

def extract_vehicle_data(json_file, image_dir):
    """Extract vehicle class data from COCO dataset with enhanced validation"""
    print(f"Extracting vehicle data from {json_file}...")

    # Load COCO annotations with validation
    coco = COCO(json_file)
    valid_classes = list(CORRECT_COCO_MAPPING.keys())

    # Get image IDs containing vehicle classes
    vehicle_img_ids = set()
    for coco_class_id in valid_classes:
        cat_img_ids = coco.getImgIds(catIds=[coco_class_id])
        vehicle_img_ids.update(cat_img_ids)

    print(f"Found {len(vehicle_img_ids)} images containing vehicle classes")

    dataset_dict = []
    original_class_counts = defaultdict(int)  # Track original COCO classes

    for img_id in tqdm(list(vehicle_img_ids)):
        img_info = coco.loadImgs(img_id)[0]
        ann_ids = coco.getAnnIds(imgIds=img_id, catIds=valid_classes, iscrowd=0)
        anns = coco.loadAnns(ann_ids)

        if not anns:
            continue

        record = {
            "file_name": os.path.join(image_dir, img_info["file_name"]),
            "image_id": img_id,
            "height": img_info["height"],
            "width": img_info["width"],
            "annotations": []
        }

        # Process annotations with enhanced validation
        for ann in anns:
            original_class = ann["category_id"]
            mapped_class = CORRECT_COCO_MAPPING.get(original_class)

            if mapped_class is None:
                continue  # Should never happen due to catIds filter

            # Track original class distribution for non-car
            if mapped_class == 1:  # non-car category
                original_class_counts[original_class] += 1

            # Validate bounding box dimensions
            x, y, w, h = ann["bbox"]
            if w < 5 or h < 5:  # More realistic minimum size
                continue

            # Validate segmentation data
            valid_segmentation = []
            if "segmentation" in ann:
                for seg in ann["segmentation"]:
                    if len(seg) >= 6:  # At least 3 points (x,y pairs)
                        valid_segmentation.append(seg)

            record["annotations"].append({
                "bbox": [float(coord) for coord in ann["bbox"]],
                "bbox_mode": BoxMode.XYWH_ABS,
                "segmentation": valid_segmentation,
                "category_id": mapped_class,
                "iscrowd": int(ann.get("iscrowd", 0))
            })

        if record["annotations"]:
            dataset_dict.append(record)

    # Enhanced class distribution analysis
    print("\nAggregated Class Distribution:")
    class_counts = defaultdict(int)
    for record in dataset_dict:
        for ann in record["annotations"]:
            class_counts[ann["category_id"]] += 1

    for cls_id, count in sorted(class_counts.items()):
        print(f"  {VEHICLE_CLASSES[cls_id]}: {count:,} instances")

    print("\nOriginal COCO Class Breakdown for Non-Car:")
    for orig_id, count in original_class_counts.items():
        class_name = coco.loadCats(orig_id)[0]["name"]
        print(f"  {class_name} (ID {orig_id}): {count:,} instances")

    return dataset_dict, class_counts

# Add additional visualization of initial class distribution before balancing
def plot_initial_class_distribution(train_class_counts, val_class_counts):
    """Plot initial class distribution for train and validation sets"""
    plt.figure(figsize=(12, 6))

    class_names = [VEHICLE_CLASSES[i] for i in range(len(VEHICLE_CLASSES))]
    train_values = [train_class_counts.get(i, 0) for i in range(len(VEHICLE_CLASSES))]
    val_values = [val_class_counts.get(i, 0) for i in range(len(VEHICLE_CLASSES))]

    x = np.arange(len(class_names))
    width = 0.35

    plt.bar(x - width/2, train_values, width, label='Training Set')
    plt.bar(x + width/2, val_values, width, label='Validation Set')

    plt.xlabel('Vehicle Classes')
    plt.ylabel('Number of Instances')
    plt.title('Initial Class Distribution in Training and Validation Sets')
    plt.xticks(x, class_names, rotation=45)
    plt.legend()

    plt.tight_layout()

    # Display the plot in the notebook
    plt.show()

    # Also save the plot
    plt.savefig(os.path.join(OUTPUT_DIR, "initial_class_distribution.png"))
    plt.close()

    print(f"Initial class distribution plot saved to {OUTPUT_DIR}/initial_class_distribution.png")

def visualize_samples(dataset_dict, num_samples=10):
    """Visualize random samples from the dataset"""
    print(f"Visualizing {num_samples} random samples...")

    samples = random.sample(dataset_dict, min(num_samples, len(dataset_dict)))

    # Create a grid for displaying multiple plots
    fig, axes = plt.subplots(nrows=min(5, num_samples), ncols=2, figsize=(20, 4*min(5, num_samples)))
    axes = axes.flatten() if num_samples > 1 else [axes]

    # Close the figure if num_samples is 0
    if num_samples < 1:
        plt.close(fig)
        return

    successful_visualizations = 0

    for i, sample in enumerate(samples):
        if i >= len(axes):
            break

        try:
            # Load image
            img = cv2.imread(sample["file_name"])
            if img is None:
                continue
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            # Plot image with annotations
            ax = axes[successful_visualizations]
            ax.imshow(img)

            # Draw bounding boxes and segmentation masks
            for ann in sample["annotations"]:
                x, y, w, h = [float(val) for val in ann["bbox"]]  # Ensure floats
                # Convert category_id to integer
                category_id = int(ann["category_id"])  # This is the fix
                class_name = VEHICLE_CLASSES[category_id]

                # Draw bounding box
                rect = plt.Rectangle((x, y), w, h,
                                    edgecolor='r', facecolor='none', linewidth=2)
                ax.add_patch(rect)

                # Add label
                ax.text(x, y-5, class_name,
                         color='white', fontsize=12,
                         bbox=dict(facecolor='red', alpha=0.7))

                # Draw segmentation if available
                if ann.get("segmentation"):
                    for seg in ann["segmentation"]:
                        # Convert points to float and reshape properly
                        poly = np.array([float(p) for p in seg]).reshape(-1, 2)
                        ax.plot(poly[:, 0], poly[:, 1], 'g-', linewidth=1.5)

            ax.set_axis_off()
            ax.set_title(f"Sample {i+1}: {os.path.basename(sample['file_name'])}")

            # Create individual plot for saving
            plt.figure(figsize=(10, 10))
            plt.imshow(img)
            for ann in sample["annotations"]:
                x, y, w, h = [float(val) for val in ann["bbox"]]
                # Convert category_id to integer here too
                category_id = int(ann["category_id"])  # This is the fix
                class_name = VEHICLE_CLASSES[category_id]

                rect = plt.Rectangle((x, y), w, h, edgecolor='r', facecolor='none', linewidth=2)
                plt.gca().add_patch(rect)
                plt.text(x, y-5, class_name, color='white', fontsize=12,
                         bbox=dict(facecolor='red', alpha=0.7))

                if ann.get("segmentation"):
                    for seg in ann["segmentation"]:
                        poly = np.array([float(p) for p in seg]).reshape(-1, 2)
                        plt.plot(poly[:, 0], poly[:, 1], 'g-', linewidth=1.5)

            plt.axis('off')
            plt.title(f"Sample {i+1}: {os.path.basename(sample['file_name'])}")
            plt.tight_layout()
            plt.savefig(os.path.join(OUTPUT_DIR, f"sample_{i+1}.png"))
            plt.close()

            successful_visualizations += 1

        except Exception as e:
            print(f"Error visualizing sample {i}: {e}")
            import traceback
            traceback.print_exc()

    # Remove unused subplots
    for j in range(successful_visualizations, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()  # Display the grid in the notebook
    plt.close(fig)

    print(f"Sample visualizations saved to {OUTPUT_DIR}")

def prepare_data_for_training(train_data, val_data):
    """Optimized data preparation with parallel processing"""
    from concurrent.futures import ThreadPoolExecutor

    def process_single_record(record):
        try:
            img_path = record["file_name"]
            img = cv2.imread(img_path)
            if img is None:
                print(f"Failed to load image: {img_path}")
                return None

            original_height, original_width = img.shape[:2]

            # Skip processing if already at target size
            if (original_height == IMG_SIZE) and (original_width == IMG_SIZE):
                return record

            # Calculate scaling factors
            width_scale = IMG_SIZE / original_width
            height_scale = IMG_SIZE / original_height

            # Resize image
            resized_img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))

            # Process annotations
            processed_annotations = []
            for ann in record["annotations"]:
                # Scale bounding box
                x, y, w, h = ann["bbox"]
                scaled_bbox = [
                    x * width_scale,
                    y * height_scale,
                    w * width_scale,
                    h * height_scale
                ]

                # Scale segmentation coordinates
                scaled_segmentation = []
                for seg in ann["segmentation"]:
                    scaled_seg = []
                    for i in range(0, len(seg), 2):
                        if i+1 < len(seg):
                            scaled_x = seg[i] * width_scale
                            scaled_y = seg[i+1] * height_scale
                            scaled_seg.extend([scaled_x, scaled_y])
                    if len(scaled_seg) >= 6:  # Keep only valid polygons
                        scaled_segmentation.append(scaled_seg)

                processed_annotations.append({
                    **ann,
                    "bbox": scaled_bbox,
                    "segmentation": scaled_segmentation
                })

            # Save resized image
            base_name = os.path.basename(img_path)
            new_path = os.path.join(OUTPUT_DIR, f"resized_{base_name}")
            cv2.imwrite(new_path, resized_img)

            return {
                **record,
                "file_name": new_path,
                "height": IMG_SIZE,
                "width": IMG_SIZE,
                "annotations": processed_annotations
            }

        except Exception as e:
            print(f"Error processing {record['file_name']}: {str(e)}")
            return None

    print("Processing training data...")
    with ThreadPoolExecutor() as executor:
        train_data = list(tqdm(executor.map(process_single_record, train_data), total=len(train_data)))

    print("Processing validation data...")
    with ThreadPoolExecutor() as executor:
        val_data = list(tqdm(executor.map(process_single_record, val_data), total=len(val_data)))

    # Filter out failed records
    train_data = [r for r in train_data if r is not None]
    val_data = [r for r in val_data if r is not None]

    print(f"Retained {len(train_data)} training samples ({len(train_data)/len(train_data):.1%})")
    print(f"Retained {len(val_data)} validation samples ({len(val_data)/len(val_data):.1%})")

    return train_data, val_data

def export_to_json(dataset_dict, output_file):
    """Export dataset to JSON format compatible with detection frameworks"""
    print(f"Exporting dataset to {output_file}...")

    # Create compatible format
    dataset = {
        "info": {
            "description": "COCO Vehicle Detection Dataset",
            "version": "1.0",
            "year": 2023,
            "contributor": "Traffic Detection Project"
        },
        "categories": [
            {"id": i, "name": name, "supercategory": "vehicle"}
            for i, name in enumerate(VEHICLE_CLASSES)
        ],
        "images": [],
        "annotations": []
    }

    annotation_id = 1

    for record in dataset_dict:
        image_id = record["image_id"]
        if isinstance(image_id, str):
            # Convert string IDs to integer hash
            image_id = hash(image_id) % 10000000

        # Add image info
        dataset["images"].append({
            "id": image_id,
            "file_name": os.path.basename(record["file_name"]),
            "height": record["height"],
            "width": record["width"]
        })

        # Add annotations
        for ann in record["annotations"]:
            x, y, w, h = BoxMode.convert(ann["bbox"], ann["bbox_mode"], BoxMode.XYWH_ABS)

            dataset["annotations"].append({
                "id": annotation_id,
                "image_id": image_id,
                "category_id": ann["category_id"],
                "bbox": [float(x), float(y), float(w), float(h)],
                "area": float(w * h),
                "segmentation": ann.get("segmentation", []),
                "iscrowd": ann.get("iscrowd", 0)
            })

            annotation_id += 1

    # Save to file
    with open(output_file, 'w') as f:
        json.dump(dataset, f)

    print(f"Dataset exported successfully: {len(dataset['images'])} images, {len(dataset['annotations'])} annotations")

def filter_small_objects(dataset_dict, min_area=100):
    """Filter out annotations with small area"""
    filtered_dataset = []

    for record in dataset_dict:
        valid_annotations = []

        for ann in record["annotations"]:
            x, y, w, h = ann["bbox"]
            area = w * h

            if area >= min_area:
                valid_annotations.append(ann)

        if valid_annotations:
            record_copy = copy.deepcopy(record)
            record_copy["annotations"] = valid_annotations
            filtered_dataset.append(record_copy)

    return filtered_dataset

def compute_class_counts(dataset_dict):
    from collections import defaultdict
    counts = defaultdict(int)
    for record in dataset_dict:
        for ann in record["annotations"]:
            counts[ann["category_id"]] += 1
    return dict(counts)

def main():
    """Main execution function"""

    # Extract vehicle data from COCO
    train_data, train_class_counts = extract_vehicle_data(TRAIN_JSON, TRAIN_DIR)
    val_data, val_class_counts = extract_vehicle_data(VAL_JSON, VAL_DIR)

    # === Filter small objects from both train and val ===
    train_data = filter_small_objects(train_data, min_area=100)
    val_data = filter_small_objects(val_data, min_area=100)

    # Recalculate class counts after filtering if needed
    train_class_counts = compute_class_counts(train_data)
    val_class_counts = compute_class_counts(val_data)

    # Plot initial class distribution
    plot_initial_class_distribution(train_class_counts, val_class_counts)

    # Prepare data for training (resize, standardize)
    train_data_processed, val_data_processed = prepare_data_for_training(train_data, val_data)

    # Export datasets to JSON format
    export_to_json(train_data_processed, os.path.join(OUTPUT_DIR, "train_vehicle_data.json"))
    export_to_json(val_data_processed, os.path.join(OUTPUT_DIR, "val_vehicle_data.json"))

    # Save dataset dictionaries for direct use with PyTorch
    torch.save(train_data_processed, os.path.join(OUTPUT_DIR, "train_vehicle_data.pt"))
    torch.save(val_data_processed, os.path.join(OUTPUT_DIR, "val_vehicle_data.pt"))

    # Visualize samples
    visualize_samples(train_data, num_samples=10)

    print("\nPreprocessing completed successfully!")
    print(f"Processed data saved to: {OUTPUT_DIR}")
    print("Files ready for model training:")
    print(f"  - {os.path.join(OUTPUT_DIR, 'train_vehicle_data.json')}")
    print(f"  - {os.path.join(OUTPUT_DIR, 'val_vehicle_data.json')}")
    print(f"  - {os.path.join(OUTPUT_DIR, 'train_vehicle_data.pt')}")
    print(f"  - {os.path.join(OUTPUT_DIR, 'val_vehicle_data.pt')}")

if __name__ == "__main__":
    main()

"""## Model Developing

# Method 01
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from torch.cuda.amp import GradScaler
from torch.cuda.amp import autocast
from torch.utils.data import Dataset, DataLoader
from torch.utils.checkpoint import checkpoint
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import cv2
import copy
import random
from torchvision import transforms
from collections import defaultdict
from PIL import Image
import math

!pip install torchinfo
from torchinfo import summary

!pip install PyTurboJPEG
from turbojpeg import TurboJPEG

jpeg = TurboJPEG()

# Set random seed for reproducibility
def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

seed_everything(42)

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Constants
IMG_SIZE = 512
NUM_CLASSES = 2
BATCH_SIZE = 32
NUM_EPOCHS = 50
LEARNING_RATE = 3e-5
WEIGHT_DECAY = 0.01
NUM_WORKERS = 4
ACCUM_STEPS = 8
PREFETCH_FACTOR = 4
NUM_QUERIES = 100
HIDDEN_DIM = 256

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Enable anomaly detection for debugging
torch.autograd.set_detect_anomaly(True)

# Dataset path constants
DATA_DIR = "/content/coco2017/vehicle_dataset"
TRAIN_DATA_PATH = os.path.join(DATA_DIR, "train_vehicle_data.pt")
VAL_DATA_PATH = os.path.join(DATA_DIR, "val_vehicle_data.pt")
OUTPUT_DIR = os.path.join(DATA_DIR, "model_output")
os.makedirs(OUTPUT_DIR, exist_ok=True)

print(f"Using device: {DEVICE}")
print(f"Training data path: {TRAIN_DATA_PATH}")
print(f"Validation data path: {VAL_DATA_PATH}")

# Import BoxMode from detectron2
try:
    from detectron2.structures import BoxMode
except ImportError:
    print("Warning: detectron2 not found, defining minimal BoxMode class")
    class BoxMode:
        """Minimal implementation of BoxMode for serialization"""
        XYXY_ABS = 0
        XYWH_ABS = 1

        @staticmethod
        def convert(*args, **kwargs):
            # Stub implementation
            return args[0]

############################
### Dataset & Data Loading
############################

class VehicleDetectionDataset(Dataset):
    def __init__(self, data_path, transform=None):
        # Define BoxMode first
        try:
            from detectron2.structures import BoxMode
        except ImportError:
            class BoxMode:
                XYXY_ABS = 0
                XYWH_ABS = 1
                @staticmethod
                def convert(*args, **kwargs):
                    return args[0]

        # Allow custom BoxMode for safe loading
        from torch.serialization import add_safe_globals
        add_safe_globals([BoxMode])

        # Load data with safety checks but allow BoxMode
        self.data = torch.load(
            data_path,
            map_location='cpu',
            weights_only=True
        )
        self.transform = transform
        self.class_names = ["car", "non-car"]
        self.num_classes = len(self.class_names)
        self.images = [os.path.join(DATA_DIR, d['file_name']) for d in self.data]
        self.annotations = [d['annotations'] for d in self.data]
        print(f"Loaded {len(self.data)} samples")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        try:
            with open(self.images[idx], 'rb') as f:
                image = jpeg.decode(f.read(), pixel_format=0)
        except Exception as e:
            print(f"Error loading image: {self.images[idx]} - {str(e)}")
            return self._create_placeholder()

        anns = self.annotations[idx]
        original_H, original_W = image.shape[:2]

        # Apply transforms
        image = self._apply_transforms(image)
        new_H, new_W = image.shape[1], image.shape[2]

        # Process annotations
        instance_masks = []
        class_labels = []
        valid_anns = []

        # Filter valid annotations
        for ann in anns:
            try:
                category_id = int(float(ann["category_id"]))
                if 0 <= category_id < self.num_classes:
                    valid_anns.append(ann)
                else:
                    print(f"Invalid category ID: {ann['category_id']} in {self.images[idx]}")
            except (KeyError, ValueError, TypeError) as e:
                print(f"Invalid annotation: {str(e)} in {self.images[idx]}")

        # Process valid annotations
        for ann in valid_anns:
            mask = self._create_instance_mask(ann, original_H, original_W)
            if mask is not None:
                instance_masks.append(mask)
                class_labels.append(int(float(ann["category_id"])))

        # Handle empty case
        if not instance_masks:
            return self._create_placeholder()

        # Convert to tensors
        instance_masks = torch.stack([m.float() for m in instance_masks])  # bool->float
        class_labels = torch.tensor(class_labels, dtype=torch.long)

        # Resize masks
        instance_masks = F.interpolate(
            instance_masks.unsqueeze(1),  # Add channel dimension
            size=(new_H, new_W),
            mode='nearest'
        ).squeeze(1)

        return image, instance_masks, class_labels


    def _create_instance_mask(self, ann, H, W):
        """Create single instance mask from annotation"""
        mask = np.zeros((H, W), dtype=np.uint8)
        if "segmentation" not in ann:
            return None

        try:
            for seg in ann["segmentation"]:
                if not isinstance(seg, (list, np.ndarray)) or len(seg) < 6:
                    continue

                poly = np.array(seg).reshape(-1, 2).astype(np.float32)
                poly = np.round(poly).astype(np.int32)
                poly[:, 0] = np.clip(poly[:, 0], 0, W-1)
                poly[:, 1] = np.clip(poly[:, 1], 0, H-1)

                if poly.shape[0] >= 3:
                    cv2.fillPoly(mask, [poly], 1)
        except Exception as e:
            print(f"Mask creation failed: {str(e)}")
            return None

        return torch.from_numpy(mask).bool() if mask.sum() > 0 else None

    def _create_placeholder(self):
        """Return empty instance masks"""
        return (
            torch.clamp(torch.zeros((3, IMG_SIZE, IMG_SIZE)), 0.0, 1.0),
            torch.zeros((0, IMG_SIZE, IMG_SIZE)),  # Empty instance masks
            torch.tensor([], dtype=torch.long)
        )

    def _apply_transforms(self, image):
        """Optimized transform pipeline"""
        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        image = transforms.Resize((IMG_SIZE, IMG_SIZE))(image)
        if self.transform:
            image = self.transform(image)
        return torch.clamp(image, 0.0, 1.0)

def collate_fn(batch):
    """Optimized collate function with safety checks"""
    # Filter invalid samples
    valid_batch = [
        (img, masks, labels)
        for img, masks, labels in batch
        if img is not None and len(labels) > 0
    ]

    # Handle empty batch
    if not valid_batch:
        return (
            torch.zeros((0, 3, IMG_SIZE, IMG_SIZE)),
            {'masks': [], 'labels': []}
        )

    # Stack images and organize targets
    images = torch.stack([item[0] for item in valid_batch])
    targets = {
        'masks': [item[1] for item in valid_batch],
        'labels': [item[2] for item in valid_batch]
    }

    return images, targets

def visualize_dataset_samples(dataset, num_samples=5, class_names=None):
    """
    Visualize sample images with instance masks and labels from the dataset

    Args:
        dataset: VehicleDetectionDataset instance
        num_samples: Number of samples to visualize
        class_names: List of class names (background + classes)
    """
    if class_names is None:
        class_names = ["background", "car", "non-car"]

    # Create color map for visualization (index 0 is background)
    colors = [
    [0, 0, 0],       # background - black
    [255, 0, 0],     # car - red
    [0, 255, 0]]      # non-car - green

    plt.figure(figsize=(15, 5*num_samples))

    for i in range(num_samples):
        # Get random sample
        idx = random.randint(0, len(dataset)-1)
        image, masks, labels = dataset[idx]

        # Denormalize image
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        denorm_image = image * std + mean
        denorm_image = denorm_image.permute(1, 2, 0).numpy()
        denorm_image = np.clip(denorm_image, 0, 1)

        # Convert tensors to numpy
        masks = masks.numpy()
        labels = labels.numpy()

        # Create subplots
        plt.subplot(num_samples, 3, i*3+1)
        plt.imshow(denorm_image)
        plt.title(f"Sample {idx}\nOriginal Image")
        plt.axis('off')

        # Plot instance masks
        plt.subplot(num_samples, 3, i*3+2)
        overlay = denorm_image.copy()
        for instance_idx in range(masks.shape[0]):
            class_id = labels[instance_idx]
            mask = masks[instance_idx]
            color = np.array(colors[class_id + 1])/255  # +1 to skip background
            overlay[mask > 0.5] = color  # Simple overwrite for visualization

        plt.imshow(overlay)
        plt.title("Instance Masks")
        plt.axis('off')

        # Plot instance boundaries
        plt.subplot(num_samples, 3, i*3+3)
        plt.imshow(denorm_image)
        for instance_idx in range(masks.shape[0]):
            class_id = labels[instance_idx]
            mask = masks[instance_idx]
            contours, _ = cv2.findContours(
                (mask > 0.5).astype(np.uint8),
                cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
            )
            for contour in contours:
                plt.plot(
                    contour[:, 0, 0], contour[:, 0, 1],
                    linewidth=2,
                    color=np.array(colors[class_id + 1])/255
                )
        plt.title("Instance Boundaries")
        plt.axis('off')

    plt.tight_layout()
    plt.show()

    # Print object instance counts per sample
    print("\nSample Object Counts:")
    for i in range(num_samples):
        _, _, labels = dataset[i]
        label_counts = {}
        for label in labels:
            class_name = class_names[label + 1]  # +1 to skip background
            label_counts[class_name] = label_counts.get(class_name, 0) + 1
        print(f"Sample {i}: {label_counts}")

############################
# Swin Transformer Backbone
############################

class PatchEmbed(nn.Module):
    """Split image into patches and embed them."""
    def __init__(self, img_size=512, patch_size=4, in_chans=3, embed_dim=96):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.n_patches = (img_size // patch_size) ** 2

        self.proj = nn.Conv2d(
            in_chans, embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        # x: B, C, H, W
        x = self.proj(x)  # B, E, H/P, W/P
        H, W = x.shape[2], x.shape[3]
        x = x.flatten(2)  # B, E, N
        x = x.transpose(1, 2)  # B, N, E
        x = self.norm(x)
        return x, H, W

class WindowAttention(nn.Module):
    """Window-based multi-head self-attention."""
    def __init__(self, dim, window_size, num_heads):
        super().__init__()
        self.dim = dim
        self.window_size = window_size
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        # Define learnable relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads)
        )

        # Get pair-wise relative position index
        coords = torch.stack(torch.meshgrid([
            torch.arange(window_size),
            torch.arange(window_size)
        ], indexing='ij'))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += window_size - 1  # shift to start from 0
        relative_coords[:, :, 1] += window_size - 1
        relative_coords[:, :, 0] *= 2 * window_size - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer("relative_position_index", relative_position_index)

        # QKV projections
        self.qkv = nn.Linear(dim, dim * 3)
        self.proj = nn.Linear(dim, dim)

        # Initialize parameters
        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)

    def forward(self, x, mask=None):
        """
        x: (B*num_windows, N, C) where N = window_size * window_size
        mask: (num_windows, Wh*Ww, Wh*Ww)
        """
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # B_, nH, N, C//nH

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))  # B_, nH, N, N

        # Add relative position bias
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size * self.window_size, self.window_size * self.window_size, -1
        )  # Wh*Ww, Wh*Ww, nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)

        attn = F.softmax(attn, dim=-1)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)

        return x

class SwinTransformerBlock(nn.Module):
    """Swin Transformer Block."""
    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4.):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio

        # Layer norm before MSA and MLP
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)

        # W-MSA/SW-MSA
        self.attn = WindowAttention(
            dim=dim,
            window_size=window_size,
            num_heads=num_heads
        )

        # MLP with GELU
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Linear(mlp_hidden_dim, dim)
        )

    def forward(self, x, H, W, mask_matrix=None):
        B, L, C = x.shape
        assert L == H * W, f"Input feature size ({L}) doesn't match H*W ({H*W})"

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # Pad if needed for window_partition
        pad_h = (self.window_size - H % self.window_size) % self.window_size
        pad_w = (self.window_size - W % self.window_size) % self.window_size
        if pad_h > 0 or pad_w > 0:
            x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h), mode='reflect')  # Pad last 3 dimensions
            _, Hp, Wp, _ = x.shape
        else:
            Hp, Wp = H, W

        # cyclic shift
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
            attn_mask = mask_matrix
        else:
            shifted_x = x
            attn_mask = None

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C

        # reverse cyclic shift
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x

        # Remove padding if added
        if pad_h > 0 or pad_w > 0:
            x = x[:, :H, :W, :].contiguous()

        x = x.view(B, H * W, C)
        # FFN
        x = shortcut + x
        x = x + self.mlp(self.norm2(x))

        return x

############################
### Window Helper Functions
############################

def window_partition(x, window_size):
    """
    Partition into non-overlapping windows.
    Pads the input if the dimensions are not divisible by the window size.
    """
    B, H, W, C = x.shape

    # Calculate padding to make dimensions divisible by window_size
    pad_h = (window_size - H % window_size) % window_size
    pad_w = (window_size - W % window_size) % window_size

    # Pad the input
    x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h), mode='constant')
    Hp, Wp = H + pad_h, W + pad_w

    # Partition into windows
    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)

    return windows

def window_reverse(windows, window_size, H, W):
    """Reverse window partition to reconstruct the original feature map."""
    # Calculate total number of windows
    nW_H = H // window_size
    nW_W = W // window_size
    # Reshape windows back into a grid of windows
    x = windows.view(-1, nW_H, nW_W, window_size, window_size, windows.shape[-1])
    # Permute dimensions and combine windows to reconstruct the original tensor
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, windows.shape[-1])
    return x

class BasicLayer(nn.Module):
    """A Swin Transformer layer for one stage"""
    def __init__(self, dim, depth, num_heads, window_size, mlp_ratio=4., downsample=None):
        super().__init__()
        self.dim = dim
        self.depth = depth
        self.window_size = window_size

        # Build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(
                dim=dim,
                num_heads=num_heads,
                window_size=window_size,
                shift_size=0 if (i % 2 == 0) else window_size // 2,
                mlp_ratio=mlp_ratio
            )
            for i in range(depth)
        ])

        # Downsample layer
        self.downsample = downsample

    def forward(self, x, H, W):
        # Create attention mask
        Hp = int(math.ceil(H / self.window_size)) * self.window_size
        Wp = int(math.ceil(W / self.window_size)) * self.window_size

        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)
        h_slices = (slice(0, -self.window_size),
                    slice(-self.window_size, -self.window_size//2),
                    slice(-self.window_size//2, None))
        w_slices = (slice(0, -self.window_size),
                    slice(-self.window_size, -self.window_size//2),
                    slice(-self.window_size//2, None))

        cnt = 0
        for h in h_slices:
            for w in w_slices:
                img_mask[:, h, w, :] = cnt
                cnt += 1

        mask_windows = window_partition(img_mask, self.window_size)
        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))

        for i, block in enumerate(self.blocks):
            x = block(x, H, W, attn_mask if block.shift_size > 0 else None) # Pass mask only if shift is applied

        if self.downsample is not None:
            x, H, W = self.downsample(x, H, W)

        return x, H, W

class PatchMerging(nn.Module):
    """Patch Merging Layer"""
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = nn.LayerNorm(4 * dim)

    def forward(self, x, H, W):
        B, L, C = x.shape
        assert L == H * W, "Input feature has wrong size"

        x = x.view(B, H, W, C)

        # Padding if needed
        pad_h = (H % 2 == 1)
        pad_w = (W % 2 == 1)
        if pad_h:
            x = F.pad(x, (0, 0, 0, 1))
        if pad_w:
            x = F.pad(x, (0, 0, 0, 0, 0, 1))

        x0 = x[:, 0::2, 0::2, :]  # B, H/2, W/2, C
        x1 = x[:, 1::2, 0::2, :]  # B, H/2, W/2, C
        x2 = x[:, 0::2, 1::2, :]  # B, H/2, W/2, C
        x3 = x[:, 1::2, 1::2, :]  # B, H/2, W/2, C
        x = torch.cat([x0, x1, x2, x3], -1)  # B, H/2, W/2, 4*C
        x = x.view(B, -1, 4 * C)  # B, H/2*W/2, 4*C

        x = self.norm(x)
        x = self.reduction(x)  # B, H/2*W/2, 2*C

        return x, (H + pad_h) // 2, (W + pad_w) // 2

class SwinTransformer(nn.Module):
    """Full Swin Transformer Backbone with v4.0 improvements"""
    def __init__(self, img_size=512, patch_size=4, in_chans=3, embed_dim=96,
                 depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],  # Deeper configuration
                 window_size=7, mlp_ratio=4., norm_layer=nn.LayerNorm):
        super().__init__()
        self.img_size = img_size
        self.embed_dim = embed_dim
        self.depths = depths
        self.num_layers = len(depths)

        # Split image into patches using convolution
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size,
            in_chans=in_chans, embed_dim=embed_dim
        )

        # Absolute position embedding
        self.pos_drop = nn.Dropout(p=0.1)

        # Build Swin Transformer layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(
                dim=int(embed_dim * 2 ** i_layer),
                depth=depths[i_layer],
                num_heads=num_heads[i_layer],
                window_size=window_size,
                mlp_ratio=mlp_ratio,
                downsample=PatchMerging(int(embed_dim * 2 ** i_layer)) if i_layer < self.num_layers-1 else None
            )
            self.layers.append(layer)

        # Final norm layer
        self.norm = norm_layer(int(embed_dim * 2 ** (self.num_layers - 1)))
        self.num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]
        self.output_features = int(embed_dim * 2 ** (self.num_layers - 1))

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x, H, W = self.patch_embed(x)
        x = self.pos_drop(x)

        features = []
        for i, layer in enumerate(self.layers):
            x, H, W = layer(x, H, W)
            # Collect features from all stages
            features.append(x.view(-1, H, W, self.num_features[i])
                          .permute(0, 3, 1, 2).contiguous())

        return features

import torch
import torch.nn as nn
import torch.nn.functional as F

############################
# Mask2Former Decoder without Fusion
############################

class Mask2FormerDecoder(nn.Module):
    def __init__(self, backbone_feature_dims, hidden_dim=256, num_queries=100, num_classes=2, img_size=512):
        """
        backbone_feature_dims: list of int, channel dimensions of each stage from the backbone
        img_size: final output mask size (H, W)
        """
        super().__init__()
        self.num_queries = num_queries
        self.img_size = img_size

        # We'll only use the last stage feature map (highest semantic, lowest spatial)
        last_dim = backbone_feature_dims[-1]

        # Project last feature map to hidden_dim
        self.input_proj = nn.Conv2d(last_dim, hidden_dim, kernel_size=1)

        # Query embeddings
        self.query_embed = nn.Embedding(num_queries, hidden_dim)

        # Transformer decoder
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=hidden_dim,
            nhead=8,
            dim_feedforward=1024,
            dropout=0.1
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)

        # Prediction heads
        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)
        self.mask_embed = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, num_queries, kernel_size=1)
        )

    def forward(self, features):
        """
        features: list of feature maps from backbone,
                  each of shape [B, C_i, H_i, W_i]
        """
        # Select only the last feature map for simplicity
        x = features[-1]  # [B, C_last, H_last, W_last]

        # Project to hidden_dim
        src = self.input_proj(x)  # [B, hidden_dim, H, W]

        # Flatten spatial dims for transformer memory
        bs, hidden, H, W = src.shape
        memory = src.flatten(2).permute(2, 0, 1)  # [H*W, B, hidden_dim]

        # Prepare queries
        query_embed = self.query_embed.weight.unsqueeze(1).repeat(1, bs, 1)  # [num_queries, B, hidden_dim]
        tgt = torch.zeros_like(query_embed)

        # Transformer decoding
        hs = self.decoder(tgt, memory)  # [num_queries, B, hidden_dim]

        # Class logits
        outputs_class = self.class_embed(hs.permute(1, 0, 2))  # [B, num_queries, num_classes+1]

        # Mask predictions
        mask_embed = self.mask_embed(src)  # [B, num_queries, H, W]
        outputs_mask = F.interpolate(
            mask_embed,
            size=(self.img_size, self.img_size),
            mode='bilinear',
            align_corners=False
        )  # [B, num_queries, img_size, img_size]

        return {'pred_logits': outputs_class,
                'pred_masks': outputs_mask}

############################
# Complete Traffic Detection Model
############################

import torch
import torch.nn as nn
import torchvision.models as models

class TrafficDetectionModel(nn.Module):
    """Enhanced Swin + Mask2Former with Feature Fusion"""
    def __init__(self, num_classes=2, img_size=512):
        super().__init__()
        self.img_size = img_size

        # Deeper Swin Backbone (v4.0 config)
        self.backbone = SwinTransformer(
            img_size=img_size,
            embed_dim=96,
            depths=[2, 2, 6, 2],  # Reduced depths for stability
            num_heads=[3, 6, 12, 24],
            window_size=7
        )

        # Feature-Enhanced Decoder (v4.0)
        self.decoder = Mask2FormerDecoder(
            backbone_feature_dims=[96, 192, 384, 768],
            hidden_dim=256,
            num_queries=100,
            num_classes=num_classes
        )

    def forward(self, x):
        """Full feature fusion forward pass"""
        # Extract multi-scale features [4 stages]
        features = self.backbone(x)

        # Pass all features to decoder (not just last)
        return self.decoder(features)

class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, ignore_index=-1, weight=None):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.ignore_index = ignore_index
        self.weight = weight

    def forward(self, inputs, targets):
        inputs = inputs.float()
        targets = targets.long()

        if inputs.numel() == 0 or targets.numel() == 0:
            return torch.tensor(0., device=inputs.device)

        ce_loss = F.cross_entropy(
            inputs,
            targets,
            reduction='none',
            ignore_index=self.ignore_index,
            weight=self.weight  # Add weight parameter
        )
        pt = torch.exp(-ce_loss)
        loss = self.alpha * (1 - pt) ** self.gamma * ce_loss

        valid_mask = targets != self.ignore_index
        if valid_mask.sum() == 0:
            return torch.tensor(0., device=inputs.device)

        return loss[valid_mask].mean()

class DiceLoss(nn.Module):
    def __init__(self, eps=1e-5):
        super().__init__()
        self.eps = eps

    def forward(self, inputs, targets):
        inputs = torch.sigmoid(inputs)
        intersection = (inputs * targets).sum()
        union = inputs.sum() + targets.sum()
        return 1 - (2. * intersection + self.eps) / (union + self.eps)

class MaskIoULoss(nn.Module):
    def __init__(self, eps=1e-7):
        super().__init__()
        self.eps = eps

    def forward(self, pred, target):
        intersection = (pred * target).sum()
        union = pred.sum() + target.sum()
        return 1 - (2 * intersection + self.eps) / (union + self.eps)

class TrafficDetectionLoss(nn.Module):
    def __init__(self, num_classes, class_weights=None, alpha=0.25, gamma=2.0, eps=1e-5):
        super().__init__()
        self.cost_class = 1.0
        self.cost_mask = 0.8
        self.cost_iou = 0.5
        if class_weights is not None:
            self.cls_loss_fn = FocalLoss(alpha, gamma, ignore_index=-1, weight=class_weights)
            self.dice_loss = DiceLoss(eps)
            self.iou_loss = MaskIoULoss(eps)


    def hungarian_matcher(self, outputs, targets):
        indices = []
        for batch_idx in range(outputs["pred_logits"].shape[0]):
            out_prob = outputs["pred_logits"][batch_idx].softmax(-1)
            out_mask = outputs["pred_masks"][batch_idx]
            tgt_mask = targets["masks"][batch_idx]
            tgt_labels = targets["labels"][batch_idx]

            # Cost matrix calculation
            cost_class = -out_prob[:, tgt_labels]
            cost_mask = torch.cdist(
                out_mask.flatten(1).sigmoid(),
                tgt_mask.flatten(1).float(),
                p=1
            )
            cost_matrix = self.cost_class * cost_class + self.cost_mask * cost_mask

            # Hungarian matching
            with torch.no_grad():
                cost_matrix_np = cost_matrix.cpu().numpy()
                row_ind, col_ind = linear_sum_assignment(cost_matrix_np)

            indices.append((row_ind, col_ind))

        return indices

    def forward(self, outputs, targets):
        # Handle empty targets
        if sum(len(t["labels"]) for t in targets) == 0:
            return {
                'loss': torch.tensor(0., device=outputs["pred_logits"].device),
                'cls_loss': torch.tensor(0.),
                'dice_loss': torch.tensor(0.),
                'iou_loss': torch.tensor(0.)
            }
        indices = self.hungarian_matcher(outputs, targets)

        # Reorder predictions and targets
        matched_outputs = {
            'pred_logits': torch.stack([outputs['pred_logits'][i][row]
                              for i, (row, _) in enumerate(indices)]),
            'pred_masks': torch.stack([outputs['pred_masks'][i][row]
                             for i, (row, _) in enumerate(indices)])
        }

        matched_targets = {
            'labels': torch.stack([targets['labels'][i][col]
                          for i, (_, col) in enumerate(indices)]),
            'masks': torch.stack([targets['masks'][i][col]
                         for i, (_, col) in enumerate(indices)])
        }

        # Loss calculations
        cls_loss = self.cls_loss_fn(matched_outputs['pred_logits'],
                                  matched_targets['labels'])
        dice_loss = self.dice_loss(matched_outputs['pred_masks'],
                                 matched_targets['masks'])
        iou_loss = self.iou_loss(matched_outputs['pred_masks'],
                               matched_targets['masks'])

        total_loss = (self.cost_class * cls_loss +
                     self.cost_mask * dice_loss +
                     self.cost_iou * iou_loss)

        return {
            'loss': total_loss,
            'cls_loss': cls_loss,
            'dice_loss': dice_loss,
            'iou_loss': iou_loss
        }

############################
# Training and Evaluation Functions
############################

def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler, accum_steps=4):
    model.train()
    progress_bar = tqdm(dataloader, desc='Training', total=len(dataloader))

    # Loss tracking
    total_loss = 0.0
    total_cls = 0.0
    total_dice = 0.0
    total_iou = 0.0

    # Training state
    accum_batches = 0
    valid_batches = 0
    train_correct = 0
    train_total = 0

    for batch_idx, (images, targets) in enumerate(progress_bar):
        # Skip empty batches
        if images.nelement() == 0 or len(targets['labels']) == 0:
            progress_bar.set_postfix({"Status": "Skipped empty batch"})
            continue

        try:
            # Device transfer with pinned memory
            images = images.pin_memory().to(device, non_blocking=True)
            targets = {
                'masks': [m.to(device, non_blocking=True) for m in targets['masks']],
                'labels': [l.to(device, non_blocking=True) for l in targets['labels']]
            }

            # Mixed precision forward
            with torch.amp.autocast(device_type=device.type, enabled=scaler is not None):
                outputs = model(images)
                loss_dict = criterion(outputs, targets)

                # Loss validation
                if torch.isnan(loss_dict['loss']) or torch.isinf(loss_dict['loss']):
                    raise ValueError("NaN/Inf detected in loss")

                loss = loss_dict['loss'] / accum_steps

            # Backpropagation
            scaler.scale(loss).backward() if scaler else loss.backward()

            # Gradient accumulation
            accum_batches += 1
            if (batch_idx + 1) % accum_steps == 0:
                # Gradient clipping
                if scaler:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

                # Optimizer step
                scaler.step(optimizer) if scaler else optimizer.step()
                optimizer.zero_grad(set_to_none=True)

                # Memory cleanup
                torch.cuda.empty_cache()
                accum_batches = 0

            # Metrics update
            total_loss += loss_dict['loss'].item()
            total_cls += loss_dict['cls_loss'].item()
            total_dice += loss_dict['dice_loss'].item()
            total_iou += loss_dict.get('iou_loss', 0).item()  # Optional

            # Accuracy calculation
            with torch.no_grad():
                preds = outputs['pred_logits'].softmax(-1).argmax(-1)
                for b in range(len(targets['labels'])):
                    valid_labels = targets['labels'][b][targets['labels'][b] != -1]
                    if valid_labels.numel() > 0:
                        train_total += len(valid_labels)
                        train_correct += (preds[b][:len(valid_labels)] == valid_labels).sum().item()

        except Exception as e:
            print(f"\nBatch {batch_idx} error: {str(e)}")
            optimizer.zero_grad(set_to_none=True)
            continue

        # Progress update
        progress_bar.set_postfix({
            "Loss": f"{total_loss/(valid_batches+1e-7):.4f}",
            "CLS": f"{total_cls/(valid_batches+1e-7):.4f}",
            "Dice": f"{total_dice/(valid_batches+1e-7):.4f}",
            "IoU": f"{total_iou/(valid_batches+1e-7):.4f}",
            "Acc": f"{(train_correct/(train_total+1e-7))*100:.1f}%",
            "LR": f"{optimizer.param_groups[0]['lr']:.2e}",
            "GPU Mem": f"{torch.cuda.memory_reserved()/1e9:.1f}G"
        })

    # Final cleanup
    if scaler: scaler.update()
    torch.cuda.empty_cache()

    return (
        total_loss/(valid_batches+1e-7),
        total_cls/(valid_batches+1e-7),
        total_dice/(valid_batches+1e-7),
        train_correct/(train_total+1e-7)
    )

from scipy.optimize import linear_sum_assignment

def validate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0.0
    cls_loss_sum = 0.0
    dice_loss_sum = 0.0
    iou_loss_sum = 0.0  # Added

    # Metrics tracking
    class_counts = torch.zeros(NUM_CLASSES, device=device)
    class_correct = torch.zeros(NUM_CLASSES, device=device)
    iou_sum = torch.zeros(NUM_CLASSES, device=device)

    with torch.no_grad(), torch.amp.autocast(device_type='cuda'):
        for images, targets in tqdm(dataloader, desc="Validation"):
            # Device transfer
            images = images.to(device, non_blocking=True)
            targets = {
                'masks': [m.to(device) for m in targets['masks']],
                'labels': [l.to(device) for l in targets['labels']]
            }

            # Forward pass
            outputs = model(images)
            loss_dict = criterion(outputs, targets)

            # Loss tracking
            total_loss += loss_dict['loss'].item()
            cls_loss_sum += loss_dict['cls_loss'].item()
            dice_loss_sum += loss_dict['dice_loss'].item()
            iou_loss_sum += loss_dict.get('iou_loss', 0).item()  # New

            # Predictions
            pred_logits = outputs['pred_logits'].softmax(-1)
            pred_masks = outputs['pred_masks'].sigmoid()

            # Per-batch processing
            for b in range(len(targets['labels'])):
                tgt_labels = targets['labels'][b]
                tgt_masks = targets['masks'][b]

                # Valid targets
                valid_mask = (tgt_labels != -1) & (tgt_labels < NUM_CLASSES)
                valid_labels = tgt_labels[valid_mask]
                valid_masks = tgt_masks[valid_mask]

                if valid_labels.numel() == 0:
                    continue

                # Hungarian matching
                cost_class = -pred_logits[b, :, valid_labels]
                cost_mask = torch.cdist(
                    pred_masks[b].flatten(1).sigmoid(),
                    valid_masks.flatten(1).float(),
                    p=1
                )
                cost = 1.0 * cost_class + 0.8 * cost_mask  # Match training weights

                # Device-safe matching
                with torch.no_grad():
                    cost_np = cost.cpu().detach().numpy()
                    row_ind, col_ind = linear_sum_assignment(cost_np)

                # Metrics calculation
                pred_classes = pred_logits[b].argmax(-1)[row_ind]
                true_classes = valid_labels[col_ind]

                # Accuracy
                correct = (pred_classes == true_classes).float()
                class_correct.scatter_add_(0, true_classes, correct)
                class_counts.scatter_add_(0, true_classes, torch.ones_like(true_classes))

                # IoU
                pred_masks_b = (pred_masks[b][row_ind] > 0.5).float()
                true_masks_b = valid_masks[col_ind]
                intersection = (pred_masks_b * true_masks_b).sum((1,2))
                union = (pred_masks_b + true_masks_b).clamp(0,1).sum((1,2))
                ious = intersection / (union + 1e-7)
                iou_sum.scatter_add_(0, true_classes, ious)

    # Safe metrics calculation
    safe_div = lambda x,y: x/y if y>0 else 0.0
    valid_classes = class_counts > 0
    total_counts = class_counts[valid_classes].sum()

    return (
        safe_div(total_loss, len(dataloader)),
        safe_div(cls_loss_sum, len(dataloader)),
        safe_div(dice_loss_sum, len(dataloader)),
        safe_div(iou_loss_sum, len(dataloader)),  # New
        safe_div(class_correct[valid_classes].sum(), total_counts),
        safe_div(iou_sum[valid_classes].sum(), total_counts)
    )

def identity(x):
    return x

def get_train_transform():
    return transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.Lambda(identity),  # Use named function instead of lambda
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

def get_val_transform():
    return transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.Lambda(identity),  # Use named function instead of lambda
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

def visualize_predictions(model, dataloader, device, num_samples=5, output_dir=None):
    """Visualize predictions from the model"""
    model.eval()

    if output_dir is None:
        output_dir = os.path.join(DATA_DIR, "visualizations")
    os.makedirs(output_dir, exist_ok=True)

    # Corrected class names and colors for 2 classes + background
    class_names = dataset.class_names
    colors = [
        [0, 0, 0],       # background - black
        [255, 0, 0],     # car - red
        [0, 255, 0]      # non-car - green
    ]

    with torch.no_grad():
        for i, (images, targets) in enumerate(dataloader):
            if i >= num_samples:
                break

            # Move data to device
            images = images.to(device)

            # Forward pass
            outputs = model(images)

            # Process predictions
            pred_logits = outputs['pred_logits']  # (B, N, C+1)
            pred_masks = outputs['pred_masks']    # (B, N, H, W)

            # Get class predictions
            pred_scores = F.softmax(pred_logits, dim=-1)
            pred_classes = pred_scores.argmax(-1)

            # Threshold masks
            pred_masks = torch.sigmoid(pred_masks) > 0.5

            # Process each image in batch
            for b in range(images.size(0)):
                # Get original image
                img = images[b].permute(1, 2, 0).cpu().numpy() * 255
                img = img.astype(np.uint8)

                # Create visualization image
                vis_img = img.copy()

                # Create target visualization
                target_vis = img.copy()
                target_masks = targets['masks'][b].cpu().numpy()  # (num_instances, H, W)
                target_labels = targets['labels'][b].cpu().numpy()  # (num_instances,)

                # Draw target masks using actual instance labels
                for instance_idx in range(len(target_labels)):
                    class_id = target_labels[instance_idx]
                    if class_id == 0:  # Skip background
                        continue

                    mask = target_masks[instance_idx] > 0.5
                    color_mask = np.zeros_like(target_vis)
                    color_mask[mask] = colors[class_id]
                    target_vis = cv2.addWeighted(target_vis, 1.0, color_mask, 0.5, 0)

                # Draw predictions
                for n in range(pred_classes.size(1)):
                    cls_id = pred_classes[b, n].item()
                    score = pred_scores[b, n, cls_id].item()

                    # Skip background and low confidence
                    if cls_id == 0 or score < 0.5:
                        continue

                    # Get mask and class info
                    mask = pred_masks[b, n].cpu().numpy()
                    class_name = class_names[cls_id]
                    color = colors[cls_id]

                    # Create colored mask
                    color_mask = np.zeros_like(vis_img)
                    color_mask[mask] = color

                    # Overlay mask
                    vis_img = cv2.addWeighted(vis_img, 1.0, color_mask, 0.5, 0)

                    # Add label
                    y, x = np.where(mask)
                    if len(y) > 0 and len(x) > 0:
                        y_min, x_min = y.min(), x.min()
                        cv2.putText(
                            vis_img, f"{class_name}: {score:.2f}",
                            (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX,
                            0.5, color.tolist(), 2
                        )

                # Save visualizations
                combined = np.hstack([img, target_vis, vis_img])
                cv2.imwrite(os.path.join(output_dir, f"sample_{i}_{b}.jpg"),
                           cv2.cvtColor(combined, cv2.COLOR_RGB2BGR))

    print(f"Visualizations saved to {output_dir}")

def visualize_dataset_samples(dataset, num_samples=5, class_names=None):
    """
    Visualize sample images with instance masks and labels from the dataset

    Args:
        dataset: VehicleDetectionDataset instance
        num_samples: Number of samples to visualize
        class_names: List of class names (background + classes) - should be ["background", ...]
    """
    # If class_names is not provided, use the dataset's names but prepend 'background'
    if class_names is None:
        class_names_with_bg = ["background"] + dataset.class_names
    else:
        # Assume provided class_names already includes background or handle accordingly
        # For safety, let's enforce the expected format if provided
        if class_names[0].lower() != 'background':
             print("Warning: Provided class_names does not start with 'background'. Assuming dataset labels are 0-indexed vehicle classes.")
             class_names_with_bg = ["background"] + class_names # Prepend background
        else:
            class_names_with_bg = class_names


    # Create color map for visualization (index 0 is background)
    colors = [
    [0, 0, 0],       # background - black
    [255, 0, 0],     # car - red (index 1 in class_names_with_bg)
    [0, 255, 0]]      # non-car - green (index 2 in class_names_with_bg)

    # Ensure color list is long enough for all classes including background
    # If you have more than 2 vehicle classes, you'd need more colors here
    while len(colors) < len(class_names_with_bg):
        # Add a default color (e.g., blue) or cycle through
        colors.append([0, 0, 255]) # Blue

    plt.figure(figsize=(15, 5*num_samples))

    for i in range(num_samples):
        # Get random sample
        idx = random.randint(0, len(dataset)-1)
        image, masks, labels = dataset[idx]

        # Denormalize image (Assuming dataset returns normalized tensor)
        # Check if image needs denormalization based on dataset __getitem__
        # The current dataset __getitem__ does not normalize, it returns [0, 1]
        # So no denormalization needed here. Just convert to numpy.
        denorm_image = image.permute(1, 2, 0).numpy() * 255
        denorm_image = denorm_image.astype(np.uint8)
        denorm_image = np.clip(denorm_image, 0, 255) # Ensure valid range after type change


        # Convert tensors to numpy
        masks = masks.numpy()
        labels = labels.numpy()

        # Create subplots
        plt.subplot(num_samples, 3, i*3+1)
        # Use OpenCV for display if available, it handles different types better
        plt.imshow(cv2.cvtColor(denorm_image, cv2.COLOR_RGB2BGR))
        plt.title(f"Sample {idx}\nOriginal Image")
        plt.axis('off')

        # Plot instance masks
        plt.subplot(num_samples, 3, i*3+2)
        overlay = denorm_image.copy()
        for instance_idx in range(masks.shape[0]):
            class_id = labels[instance_idx] # This is the dataset's 0-indexed ID (0 or 1)
            mask = masks[instance_idx]
            # Use class_id directly + 1 for index in class_names_with_bg and colors
            color_index = class_id + 1
            color = np.array(colors[color_index])  # Use color from the extended list
            # Apply color where mask is > 0.5 (or > 0 if mask is boolean/uint8)
            overlay[mask > 0] = color

        # Need to convert back to RGB for matplotlib display
        plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))
        plt.title("Instance Masks")
        plt.axis('off')

        # Plot instance boundaries
        plt.subplot(num_samples, 3, i*3+3)
        plt.imshow(cv2.cvtColor(denorm_image, cv2.COLOR_RGB2BGR))
        for instance_idx in range(masks.shape[0]):
            class_id = labels[instance_idx] # This is the dataset's 0-indexed ID (0 or 1)
            mask = masks[instance_idx]
            color_index = class_id + 1 # Use color from the extended list
            color = np.array(colors[color_index]).tolist() # Get color as list for cv2

            # Find contours on the mask (make sure mask is binary)
            contours, _ = cv2.findContours(
                (mask > 0).astype(np.uint8), # Ensure binary mask
                cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
            )
            # Draw contours on the denormalized image copy
            cv2.drawContours(denorm_image, contours, -1, color, 2) # Draw contours

        # Display the image with contours
        plt.imshow(cv2.cvtColor(denorm_image, cv2.COLOR_RGB2BGR))
        plt.title("Instance Boundaries")
        plt.axis('off')

    plt.tight_layout()
    plt.show()

    # Print object instance counts per sample
    print("\nSample Object Counts:")
    for i in range(num_samples):
        # Get random sample index again if needed, or iterate through the first num_samples
        # Using the same random index as before might be confusing if samples are non-contiguous
        # Let's re-get a sample for counting
        idx = random.randint(0, len(dataset)-1) # Use new random index for counts
        _, _, labels = dataset[idx] # Get only labels for counting
        label_counts = {}
        for label in labels:
            # Map dataset label (0, 1) to the name in the class_names_with_bg list
            # label 0 (car) -> index 1 ("car")
            # label 1 (non-car) -> index 2 ("non-car")
            class_name = class_names_with_bg[label + 1]
            label_counts[class_name] = label_counts.get(class_name, 0) + 1
        print(f"Sample {idx} counts: {label_counts}") # Print the actual index used

# Define transformation for validation data
val_transform = get_val_transform()

# Create validation dataset
# Assuming VAL_DATA_PATH and DATA_DIR are defined in a previous cell
val_dataset = VehicleDetectionDataset(
    data_path=VAL_DATA_PATH,
    transform=val_transform
)

# Visualize validation samples with annotations
# Pass the extended class_names list including 'background'
# We can create this list explicitly or let the function handle it based on the dataset's class_names
extended_class_names = ["background"] + train_dataset.class_names
visualize_dataset_samples(train_dataset, num_samples=10, class_names=extended_class_names)

extended_class_names = ["background"] + val_dataset.class_names
visualize_dataset_samples(val_dataset, num_samples=10, class_names=extended_class_names)

def plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, val_ious):
    """Updated to handle validation IoU"""
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))

    # Loss plot
    ax1.plot(train_losses, 'b-', label='Train')
    ax1.plot(val_losses, 'r-', label='Val')
    ax1.set_title('Loss Curves')
    ax1.legend()

    # Accuracy plot
    ax2.plot(train_accuracies, 'g-', label='Train')
    ax2.plot(val_accuracies, 'm-', label='Val')
    ax2.set_title('Accuracy Curves')
    ax2.legend()

    # IoU plot
    ax3.plot(val_ious, 'y-', label='Val IoU')
    ax3.set_title('Validation IoU')
    ax3.legend()

    plt.savefig(os.path.join(OUTPUT_DIR, 'training_metrics.png'))

############################
### Dataset Verification
############################

def verify_dataset(dataset, num_samples=5):
    print("\nDataset Verification:")
    for i in range(num_samples):
        image, masks, labels = dataset[i]

        # Updated checks
        assert image.shape == (3, IMG_SIZE, IMG_SIZE), f"Invalid image shape: {image.shape}"
        assert masks.dim() == 3, f"Invalid mask dims: {masks.shape} (should be [instances, H, W])"
        assert labels.dim() == 1, f"Invalid labels shape: {labels.shape}"

        # Content validation
        if len(labels) > 0:  # Only check if instances exist
            assert masks.shape[0] == len(labels), "Masks/Labels count mismatch"
            assert torch.max(labels) < NUM_CLASSES, f"Invalid label: {torch.max(labels)}"

    print("Dataset verification passed!")

# Load validation data directly, setting `weights_only` to `False`
from torch.serialization import add_safe_globals
from detectron2.structures import BoxMode
add_safe_globals([BoxMode])
val_data = torch.load(VAL_DATA_PATH, map_location='cpu', weights_only=True)

# Check annotation counts
empty_samples = 0
total_anns = 0

for idx, sample in enumerate(val_data):
    valid_anns = [
        ann for ann in sample['annotations']
        if 0 <= int(float(ann["category_id"])) < NUM_CLASSES
    ]
    if len(valid_anns) == 0:
        empty_samples += 1
    total_anns += len(valid_anns)

print(f"Validation Set Report:")
print(f"Total samples: {len(val_data)}")
print(f"Samples with 0 instances: {empty_samples} ({empty_samples/len(val_data):.1%})")
print(f"Total instances: {total_anns}")
print(f"Avg instances per sample: {total_anns/len(val_data):.1f}")

def save_checkpoint(model, optimizer, scaler, metrics, name):
    torch.save({
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict() if optimizer else None,
        'scaler': scaler.state_dict() if scaler else None,
        'metrics': dict(metrics)
    }, os.path.join(OUTPUT_DIR, f'{name}_model.pth'))

class EarlyStopper:
    def __init__(self, patience=5, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_loss = float('inf')

    def step(self, current_loss):
        if current_loss < (self.min_loss - self.min_delta):
            self.min_loss = current_loss
            self.counter = 0
        else:
            self.counter += 1
        return self.counter >= self.patience

def adjust_resolution(model, scale_factor=1.5):
    """Safe resolution adjustment for DataParallel/normal models"""
    if hasattr(model, 'module'):
        # DataParallel case
        model.module.img_size = int(model.module.img_size * scale_factor)
        model.module.backbone.img_size = model.module.img_size
    else:
        # Single GPU case
        model.img_size = int(model.img_size * scale_factor)
        model.backbone.img_size = model.img_size
    print(f"Resolution adjusted to {model.module.img_size if hasattr(model, 'module') else model.img_size}")

def compute_class_weight(class_weight, classes, y):
    """
    Mimics sklearn.utils.class_weight.compute_class_weight
    for "balanced" strategy in PyTorch environment
    """
    # Convert to numpy arrays for processing
    y = np.asarray(y)
    classes = np.asarray(classes)

    # Calculate class frequencies
    class_counts = np.bincount(y[y >= 0])  # Ignore negative labels

    if len(class_counts) != len(classes):
        raise ValueError("Number of classes doesn't match class counts")

    # Calculate balanced weights
    n_samples = len(y)
    n_classes = len(classes)
    weights = n_samples / (n_classes * class_counts.astype(np.float32))

    # Normalize weights to sum to n_classes
    weights = weights / np.sum(weights) * n_classes

    return weights.astype(np.float32)

############################
# Main Training Loop
############################

def main():
    # Setup and diagnostics
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    seed_everything(42)
    print(f"Using device: {DEVICE}")

    # Class-balanced initialization (v4.0)
    print("\nComputing class weights...")
    train_data = torch.load(TRAIN_DATA_PATH)

    # Flatten all labels from annotations
    all_labels = torch.cat([
        torch.tensor([ann['category_id']
        for ann in d['annotations'] if len(d['annotations']) > 0])
        for d in train_data
    ])

    # Get unique classes present in dataset
    classes = np.unique(all_labels.numpy())

    # Compute weights
    class_weights = compute_class_weight(
        'balanced',
        classes=classes,
        y=all_labels.numpy()
    )

    # Convert to tensor and move to device
    class_weights = torch.tensor(class_weights, dtype=torch.float32, device=DEVICE)

    # v3.0 Data Loading with optimizations
    print("\nLoading datasets...")
    train_dataset = VehicleDetectionDataset(TRAIN_DATA_PATH, transform=get_train_transform())
    val_dataset = VehicleDetectionDataset(VAL_DATA_PATH, transform=get_val_transform())

    # Fix 1: Verify class names match 2-class setup
    train_dataset.class_names = ["car", "non-car"]
    val_dataset.class_names = ["car", "non-car"]

    verify_dataset(train_dataset)
    verify_dataset(val_dataset)

    # Maintain v3.0 DataLoader config
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        persistent_workers=False,
        collate_fn=collate_fn,
        drop_last=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        persistent_workers=False,
        collate_fn=collate_fn
    )

    # v4.0 Model Initialization
    print("\nInitializing model...")
    model = TrafficDetectionModel(num_classes=NUM_CLASSES, img_size=IMG_SIZE)

    # Fix 2: Explicit backbone feature dimensions
    model.decoder = Mask2FormerDecoder(
        backbone_feature_dims=[96, 192, 384, 768],  # Match Swin-T output channels
        hidden_dim=256,
        num_queries=100,
        num_classes=NUM_CLASSES
    )

    model = nn.DataParallel(model).to(DEVICE) if torch.cuda.device_count() > 1 else model.to(DEVICE)

    # v4.0 Model Summary
    summary(
        model,
        input_size=(1, 3, IMG_SIZE, IMG_SIZE),
        col_names=("input_size", "output_size", "num_params"),
        verbose=1
    )

    # v4.0 Enhanced Loss with empty target handling
    criterion = TrafficDetectionLoss(
        num_classes=NUM_CLASSES,
        class_weights=class_weights,
        alpha=0.25,
        gamma=2.0,
        eps=1e-7
    ).to(DEVICE)

    # v4.0 Optimizer Configuration
    optimizer = optim.AdamW(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        betas=(0.9, 0.999),
        eps=1e-8
    )

    # v4.0 Progressive Learning Schedule
    scheduler = CosineAnnealingWarmRestarts(
        optimizer,
        T_0=10,
        T_mult=2,
        eta_min=1e-6,
        last_epoch=-1
    )

    # v4.0 Mixed Precision
    scaler = GradScaler(
        init_scale=2.**14,
        growth_interval=2000,
        enabled=True
    )

    # v4.0 Training State Tracking
    metrics_history = defaultdict(list)
    early_stopper = EarlyStopper(patience=5, min_delta=0.001)

    # ---- Training Loop ----
    print("\nStarting training...")
    for epoch in range(NUM_EPOCHS):
        # v4.0 Dynamic Resolution (Optional)
        if epoch == 15 and not isinstance(model, nn.DataParallel):
            adjust_resolution(model, scale_factor=1.5)
            print(f"| Resolution increased to {model.img_size}")

        # Fix 3: Add empty batch handling in training
        train_metrics = train_one_epoch(
            model, train_loader, optimizer, criterion,
            DEVICE, scaler, ACCUM_STEPS  # Fixed argument order
        )

        # Fix 4: Add validation empty target handling
        val_metrics = validate(model, val_loader, criterion, DEVICE)

        # Update metrics (v4.0 format)
        for k,v in zip(['loss', 'cls', 'dice', 'acc'], train_metrics):
            metrics_history[f'train_{k}'].append(v)

        metric_names = ['loss', 'cls', 'dice', 'iou_loss', 'acc', 'iou']
        for name, value in zip(metric_names, val_metrics):
            metrics_history[f'val_{name}'].append(value)

        # v4.0 Learning Rate Schedule
        scheduler.step()

        # Fix 5: Checkpoint based on validation IoU
        if len(metrics_history['val_iou']) == 0 or val_metrics[-1] > max(metrics_history['val_iou']):
            save_checkpoint(model, optimizer, scaler, metrics_history, 'best')

        # v4.0 Early Stopping
        if early_stopper.step(val_metrics[0]):
            print(f"Early stopping at epoch {epoch+1}")
            break

    # v3.0 Finalization with v4.0 Metrics
    save_checkpoint(model, None, None, metrics_history, 'final')
    plot_training_curves(
        metrics_history['train_loss'],
        metrics_history['val_loss'],
        metrics_history['train_acc'],
        metrics_history['val_acc'],
        metrics_history['val_iou']
    )
    visualize_predictions(model, val_loader, DEVICE)
    print(f"Training complete. Results in {OUTPUT_DIR}")

if __name__ == "__main__":
    main()

# Load metrics from best model
checkpoint = torch.load(os.path.join(OUTPUT_DIR, 'best_model.pth'))
val_metrics = checkpoint['metrics']['val']
train_metrics = checkpoint['metrics']['train']

# Plot curves
plot_training_curves(
    train_metrics['loss'],
    val_metrics['loss'],
    train_metrics['acc'],
    val_metrics['acc'],
    val_metrics['iou']
)

# Disconnect runtime
import os
os.kill(os.getpid(), 9)

